mode: train
resume: False
ckpt_path: outputs/2022-06-21/17-33-40/iter_174.pt

gpus: [0,1,2,3]
ddp_port: 23333
omp_num_threads: 1

seed: 98765
wandb:
  en: True
  offline: False
  entity: quartz
  project: PPO-Pretrain

# quartz
input_graphs:
  - name: 'barenco_tof_3'
    path: '../t_tdg_h_cx_toffoli_flip_dataset/barenco_tof_3.qasm.toffoli_flip' # '../barenco_tof_3_opt_path/subst_history_39.qasm'
  # - name: 'barenco_tof_3'
  #   path: '../barenco_tof_3_opt_path/subst_history_39.qasm'
  # mod5_4
  # 

gate_set: ['h', 'cx', 'x', 'rz', 'add'] # ['h', 'cx', 't', 'tdg']
ecc_file: 'Nam_complete_ECC_set.json' # 'bfs_verified_simplified.json'
no_increase: False
include_nop: True

# network
graph_embed_size: 128
actor_hidden_size: 256
critic_hidden_size: 128

# algorithm
gamma: 0.95
entropy_coeff: 0.02
eps_clip: 0.2
softmax_temp:
  en: True
  hit_rate: 0.9

# multiprocessing
mp_start_method: spawn # fork
obs_per_agent: 3 # 128

# exp collection
nop_stop: True
invalid_reward: -1.0
max_gate_count_ratio: 1.2
batch_inference: True
agent_collect: False
dyn_eps_len: True
max_eps_len: 300
min_eps_len: 20

# training
max_iterations: 1e8
num_eps_per_iter: 30
mini_batch_size: 3600 # per DDP process; < num_eps_per_iter * len_episode
k_epochs: 25
lr_graph_embedding: 3e-4
lr_actor: 3e-4
lr_critic: 5e-4
update_policy_interval: 1
save_ckpt_interval: 5
